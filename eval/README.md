# Eval æ¡†æ¶ - çº¯è§„åˆ™å¿«é€ŸéªŒè¯

> ä¸éœ€è¦ LLMï¼Œå®Œå…¨åŸºäºè§„åˆ™æ£€æŸ¥ï¼Œå…è´¹ä¸”å¿«é€Ÿ

---

## ğŸš€ å¿«é€Ÿå¼€å§‹

### æœ€ç®€å•çš„ç”¨æ³•ï¼ˆä¸€è¡Œä»£ç ï¼‰

```python
from eval.rule_evaluator import is_quality_ok

if is_quality_ok(ai_output):
    print("è´¨é‡è¾¾æ ‡ âœ…")
else:
    print("è´¨é‡ä¸è¾¾æ ‡ âŒ")
```

### è·å–è¯¦ç»†è¯„ä¼°ç»“æœ

```python
from eval.rule_evaluator import quick_eval

result = quick_eval(ai_output)

print(f"é€šè¿‡: {result['passed']}")
print(f"å¾—åˆ†: {result['score']}")
print(f"æ‘˜è¦: {result['summary']}")
print(f"å¤±è´¥é¡¹: {result['failed_checks']}")
```

### å®Œæ•´ç”¨æ³•

```python
from eval import RuleEvaluator

# åˆ›å»ºè¯„ä¼°å™¨ï¼ˆå¯è‡ªå®šä¹‰é˜ˆå€¼ï¼‰
evaluator = RuleEvaluator(threshold=0.7)

# è¯„ä¼°
result = evaluator.evaluate(
    text=ai_output,
    context="ç”¨æˆ·çš„åŸå§‹è¾“å…¥"  # å¯é€‰
)

# ç¾åŒ–æ‰“å°
evaluator.print_result(result)
```

---

## ğŸ“Š æ£€æŸ¥é¡¹è¯´æ˜

### ç»“æ„æ£€æŸ¥ (Structure)

| æ£€æŸ¥é¡¹ | è¯´æ˜ | é€šè¿‡æ¡ä»¶ |
|-------|------|---------|
| `has_titles` | æ˜¯å¦æœ‰æ ‡é¢˜ç»“æ„ | â‰¥2 ä¸ª # æ ‡é¢˜ |
| `has_sections` | æ˜¯å¦æœ‰å¿…è¦ç« èŠ‚ | åŒ…å«æ‘˜è¦/å¼•è¨€/ç»“è®ºç­‰ |
| `paragraph_structure` | æ®µè½ç»“æ„åˆç† | 5-50 æ®µï¼Œå¹³å‡ 50-500 å­— |

### å†…å®¹æ£€æŸ¥ (Content)

| æ£€æŸ¥é¡¹ | è¯´æ˜ | é€šè¿‡æ¡ä»¶ |
|-------|------|---------|
| `min_length` | æœ€å°é•¿åº¦ | â‰¥500 å­— |
| `has_evidence` | æœ‰è¯æ®æ”¯æ’‘ | â‰¥2 ä¸ªè¯æ®å…³é”®è¯ |
| `has_data` | æœ‰æ•°æ® | â‰¥1 å¤„æ•°æ® |
| `no_placeholder` | æ— å ä½ç¬¦ | æ—  [TODO] ç­‰ |

### è´¨é‡æ£€æŸ¥ (Quality)

| æ£€æŸ¥é¡¹ | è¯´æ˜ | é€šè¿‡æ¡ä»¶ |
|-------|------|---------|
| `no_repetition` | æ— é‡å¤ | é‡å¤ç‡ <10% |
| `readability` | å¯è¯»æ€§ | å¹³å‡å¥é•¿ 20-80 å­— |
| `terminology` | ä¸“ä¸šæœ¯è¯­ | æœ¯è¯­è¦†ç›– â‰¥20% |

---

## ğŸ“ˆ è¯„åˆ†è§„åˆ™

- æ¯é¡¹æ£€æŸ¥å¾—åˆ† 0-1
- æ€»åˆ† = æ‰€æœ‰æ£€æŸ¥é¡¹çš„å¹³å‡åˆ†
- é»˜è®¤é€šè¿‡é˜ˆå€¼ = 0.7

---

## ğŸ”§ è‡ªå®šä¹‰é…ç½®

### ä¿®æ”¹é€šè¿‡é˜ˆå€¼

```python
# ä¸¥æ ¼æ¨¡å¼
evaluator = RuleEvaluator(threshold=0.8)

# å®½æ¾æ¨¡å¼
evaluator = RuleEvaluator(threshold=0.6)
```

### åªè¿è¡Œéƒ¨åˆ†æ£€æŸ¥

```python
from eval.checkers import StructureChecker, ContentChecker

# åªæ£€æŸ¥ç»“æ„
structure = StructureChecker()
results = structure.run_all(text)

# åªæ£€æŸ¥å†…å®¹
content = ContentChecker()
results = content.run_all(text)
```

---

## ğŸ“ æ–‡ä»¶ç»“æ„

```
eval/
â”œâ”€â”€ __init__.py          # å¯¼å‡ºæ¥å£
â”œâ”€â”€ checkers.py          # ä¸‰ç±»æ£€æŸ¥å™¨
â”œâ”€â”€ rule_evaluator.py    # ä¸»è¯„ä¼°å™¨
â””â”€â”€ README.md            # æœ¬æ–‡ä»¶
```

---

## ğŸ’¡ ä½¿ç”¨åœºæ™¯

1. **CI/CD é›†æˆ**ï¼šä½œä¸ºè´¨é‡é—¨ç¦
2. **å¼€å‘è‡ªæµ‹**ï¼šå¿«é€ŸéªŒè¯ AI è¾“å‡º
3. **æ‰¹é‡ç­›é€‰**ï¼šè¿‡æ»¤ä½è´¨é‡è¾“å‡º
4. **é¢„æ£€**ï¼šåœ¨è°ƒç”¨ LLM è¯„ä¼°å‰å…ˆåšè§„åˆ™æ£€æŸ¥

---

## ğŸ¯ ä¸ pytest é›†æˆ

```python
# tests/test_eval.py

import pytest
from eval import RuleEvaluator

evaluator = RuleEvaluator(threshold=0.7)

def test_output_quality():
    ai_output = get_ai_output()
    result = evaluator.evaluate(ai_output)
    
    assert result["passed"], f"è´¨é‡ä¸è¾¾æ ‡: {result['failed_checks']}"
    assert result["score"] >= 0.7, f"å¾—åˆ†è¿‡ä½: {result['score']}"
```

---

## ğŸ“Š è¾“å‡ºç¤ºä¾‹

```
==================================================
è¯„ä¼°ç»“æœ: âœ… é€šè¿‡
æ€»åˆ†: 0.85 (é˜ˆå€¼: 0.7)
æ‘˜è¦: é€šè¿‡ 8/10 é¡¹æ£€æŸ¥ï¼Œå¾—åˆ† 0.85
==================================================

âŒ æœªé€šè¿‡çš„æ£€æŸ¥:
  - has_data: ç¼ºå°‘æ•°æ®æ”¯æ’‘
  - terminology: æœ¯è¯­è¦†ç›– 15%

âœ… é€šè¿‡çš„æ£€æŸ¥: has_titles, has_sections, paragraph_structure, min_length, has_evidence, no_placeholder, no_repetition, readability
```

